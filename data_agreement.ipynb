{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "132a8abbad3b4b97aa2bdfffce7113f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DropdownModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "/content/drive/MyDrive/PhD/Toxic Reasoning/Evaluation/ChatGPT Test set predictions/gpt4o_2025-01-22_12-03-30"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_3a27d001507946399c9da16d59b6a796",
      "style": "IPY_MODEL_fb7d1eee2a34499096aa24ac3ad83dcc"
     }
    },
    "3a27d001507946399c9da16d59b6a796": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb7d1eee2a34499096aa24ac3ad83dcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:01.755273Z",
     "start_time": "2025-03-31T09:18:01.744186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import itertools\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters\n",
    "\n",
    "from data import _preprocess, COLUMNS"
   ],
   "metadata": {
    "id": "SkHCd6FqPruC",
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:04.603226Z",
     "start_time": "2025-03-31T09:18:01.800198Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LOAD TEST SET"
   ],
   "metadata": {
    "id": "jEmbW8B35Wib"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:06.966153Z",
     "start_time": "2025-03-31T09:18:04.685691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TEST_DATA_FILE_NAME = '../data/temporal/preprocessed_test.pkl'\n",
    "test_df = _preprocess(pd.read_pickle(TEST_DATA_FILE_NAME))\n",
    "\n",
    "# TODO fix this upstream somewhere\n",
    "test_df['answer_pp_toxicity'] = test_df['answer_toxicity']\n",
    "\n",
    "# extract language from filename\n",
    "test_df['lang'] = test_df['filename'].str.extract(r'batch_(..)_\\d+\\.csv')\n",
    "test_df['lang'].unique()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nl', 'en', 'es', 'tr', 'ar', 'de'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### LOAD GPT ANNOTATIONS"
   ],
   "metadata": {
    "id": "Xvp-eNqJ4tF1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TEST_PRED_FOLDER = '../data/temporal/gpt4o_2025-01-22_12-03-30'\n",
    "\n",
    "gpt_df = pd.concat([pd.read_csv(os.path.join(TEST_PRED_FOLDER, f)) for f in os.listdir(TEST_PRED_FOLDER) if f.endswith('.csv')])\n",
    "\n",
    "# expand answer field\n",
    "df_answers = gpt_df['answer'].apply(json.loads).apply(json.loads).apply(pd.Series).add_prefix('answer_')\n",
    "df_answers = df_answers.where(df_answers.astype(bool), np.nan)  # removes empty dictionaries and replaces with NaN\n",
    "gpt_df = pd.concat([gpt_df.drop(columns='answer'), df_answers], axis=1)\n",
    "\n",
    "# split up trinary\n",
    "gpt_df['answer_toxicity'] = gpt_df['answer_trinary'].apply(lambda a: 'Yes/Maybe' if a['_Yes/Maybe'] else 'No')\n",
    "gpt_df['answer_counternarrative'] = gpt_df['answer_trinary'].apply(lambda a: 'Yes' if a['_Counter-speech'] else 'No')\n",
    "\n",
    "# further preprocessing\n",
    "gpt_df = _preprocess(gpt_df)\n",
    "gpt_df['workerid'] = 'gpt4o'\n",
    "# pred_df['answer_implTopic'] = pred_df['answer_implTopic'].str.extract(r'^(?:\\.\\.\\.\\s)?(\\(..?.?\\))')\n"
   ],
   "metadata": {
    "id": "4YL_wJqUPmmc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8f8154c2-aa9b-4584-e12b-85197990d27f",
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:07.961933Z",
     "start_time": "2025-03-31T09:18:07.075959Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:08.006488Z",
     "start_time": "2025-03-31T09:18:07.975404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Langauge misidentifications (See https://docs.google.com/spreadsheets/d/1K7AHawqNgLOV4SryJGr_BbQFwcB8K4gPOR9Xattx5Q4)\n",
    "MISIDENTIFIED_THREAD_IDS = {\n",
    "    '60761694404071785', '5687396952074376', '48407333198278932', '53501753679094372',\n",
    "    '54288785178874084', '66024864922700717', '71880017305366941', '16353993833502616',\n",
    "    '43223883605944201', '50032915589739384', '52097527239380956',\n",
    "}\n",
    "gpt_df = gpt_df[~gpt_df['st_id'].isin(MISIDENTIFIED_THREAD_IDS)]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### COMBINE"
   ],
   "metadata": {
    "id": "6ZN5YplUasMl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.concat([test_df, gpt_df])\n",
    "df.shape"
   ],
   "metadata": {
    "id": "hmUUgjq2TDH2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "afbca4a7-92a7-44d2-dcef-cbfafe31e108",
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:08.072688Z",
     "start_time": "2025-03-31T09:18:08.028170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11630, 115)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:09.323470Z",
     "start_time": "2025-03-31T09:18:08.149492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.to_csv('agreement_data.csv')"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-processing"
   ],
   "metadata": {
    "id": "bdTDXxIIe78D"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add field for 'did someone fill out an implication'\n"
   ],
   "metadata": {
    "id": "lEkVZANQztCc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "impl1 = df['answer_pp_englishImplication'].str.len() > 0\n",
    "impl2 = df['answer_pp_implication'].str.len() > 0\n",
    "impl3 = df['answer_pp_nativeImplication'].str.len() > 0\n",
    "df['nonEmptyImplication'] = impl1 | impl2 | impl3"
   ],
   "metadata": {
    "id": "HMzLge5hzsTr",
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:09.362056Z",
     "start_time": "2025-03-31T09:18:09.336679Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export for manual inspection"
   ],
   "metadata": {
    "id": "PI4P1wH6fHPG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMYiEaRPgukH",
    "outputId": "5603ade4-754a-432c-e3f5-595b3ec25ebe",
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:09.417504Z",
     "start_time": "2025-03-31T09:18:09.394378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11630, 116)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# df.pivot(\n",
    "#     columns=['workerid'], index=['comment_id'], values=['answer_implication']\n",
    "# ).to_csv('implications.csv')\n",
    "# df.pivot(\n",
    "#     columns=['workerid'], index=['st_id', 'st_nr', 'comment_body'], values=['answer_toxicity']\n",
    "# ).to_csv('first_question_a.csv')\n",
    "# df.pivot(\n",
    "#     columns=['workerid'], index=['st_id', 'st_nr', 'comment_body'], values=['answer_counternarrative']\n",
    "# ).to_csv('first_question_b.csv')\n",
    "# df.pivot(\n",
    "#     columns=['workerid'], index=['comment_id'], values=['answer_justInappropriate']\n",
    "# ).to_csv('second_question.csv')\n",
    "# df.pivot(\n",
    "#     columns=['workerid', 'comment_body'], index=['comment_id'], values=['answer_implication']\n",
    "# ).to_csv('implications_with_comments.csv')"
   ],
   "metadata": {
    "id": "-a8LW-WdUQLB",
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:09.574300Z",
     "start_time": "2025-03-31T09:18:09.550909Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ..."
   ],
   "metadata": {
    "id": "FX9h1wHgFaRZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def conditional_calculation(all_calcs, do_calc):\n",
    "    for calc in all_calcs:\n",
    "        suitable_comment_ids = set(df['comment_id'].tolist())\n",
    "        for args in calc['conditions']:\n",
    "            condition, *args = args\n",
    "            if condition == 'full_agreement':\n",
    "                # create list of comment ids where each annotator has the same answer (and it is not NaN)\n",
    "                column = args\n",
    "                agreed = df.groupby(by=['comment_id'])['answer_pp_' + column].nunique() == 1\n",
    "                comment_ids = set(agreed.index[agreed])\n",
    "            elif condition == 'full_agreement_on_value':\n",
    "                # create list of comment ids where each annotator has a specific answer\n",
    "                column, value = args\n",
    "                def mapping(x):\n",
    "                    if column not in COLUMNS:\n",
    "                        return x\n",
    "                    return COLUMNS[column].apply_fn(x)\n",
    "\n",
    "                correct = df.groupby(by=['comment_id'])['answer_pp_' + column].apply(\n",
    "                    lambda grp: (grp.map(mapping) == value).all()\n",
    "                )\n",
    "                # print(correct.index[correct])\n",
    "                comment_ids = set(correct.index[correct])\n",
    "                # print(comment_ids)\n",
    "            elif condition == 'column_in_list':\n",
    "                column, values = args\n",
    "                comment_ids = set(df[df[column].isin(values)]['comment_id'].tolist())\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            suitable_comment_ids = suitable_comment_ids & comment_ids\n",
    "\n",
    "        # print(f'nr of suitable ids: {len(suitable_comment_ids)}')\n",
    "        # comments that meet conditions\n",
    "        suitable_df = df[df['comment_id'].isin(suitable_comment_ids)].sort_values('comment_id')\n",
    "        # print(f'nr of suitable rows: {suitable_df.shape}')\n",
    "\n",
    "        for col in calc['to_calculate']:\n",
    "            # print(calc['prefix'])\n",
    "            do_calc(suitable_df, col, calc['prefix'], calc['labels'])"
   ],
   "metadata": {
    "id": "OvdI2P7nFbYe",
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:09.663894Z",
     "start_time": "2025-03-31T09:18:09.634330Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate Agreements"
   ],
   "metadata": {
    "id": "ROqkdcLefCLu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.groupby('filename')['workerid'].apply(lambda grp: grp.value_counts().shape).reset_index(name='Nr')"
   ],
   "metadata": {
    "id": "ONSnAh5EbNB7",
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:09.737020Z",
     "start_time": "2025-03-31T09:18:09.695116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            filename    Nr\n",
       "0     batch_ar_1.csv  (4,)\n",
       "1    batch_ar_10.csv  (4,)\n",
       "2    batch_ar_11.csv  (4,)\n",
       "3    batch_ar_12.csv  (4,)\n",
       "4    batch_ar_13.csv  (4,)\n",
       "..               ...   ...\n",
       "110   batch_tr_5.csv  (4,)\n",
       "111   batch_tr_6.csv  (4,)\n",
       "112   batch_tr_7.csv  (4,)\n",
       "113   batch_tr_8.csv  (4,)\n",
       "114   batch_tr_9.csv  (4,)\n",
       "\n",
       "[115 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Nr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>batch_ar_1.csv</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>batch_ar_10.csv</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>batch_ar_11.csv</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batch_ar_12.csv</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>batch_ar_13.csv</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>batch_tr_5.csv</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>batch_tr_6.csv</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>batch_tr_7.csv</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>batch_tr_8.csv</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>batch_tr_9.csv</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "df.groupby('filename')['workerid'].apply(lambda grp: grp.unique()).reset_index()"
   ],
   "metadata": {
    "id": "Svvoi-ovXr0v",
    "ExecuteTime": {
     "end_time": "2025-03-31T09:18:09.927638Z",
     "start_time": "2025-03-31T09:18:09.878425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            filename                             workerid\n",
       "0     batch_ar_1.csv  [husseinsarrar, ilham, nada, gpt4o]\n",
       "1    batch_ar_10.csv  [husseinsarrar, ilham, nada, gpt4o]\n",
       "2    batch_ar_11.csv  [husseinsarrar, ilham, nada, gpt4o]\n",
       "3    batch_ar_12.csv  [husseinsarrar, ilham, nada, gpt4o]\n",
       "4    batch_ar_13.csv  [husseinsarrar, ilham, nada, gpt4o]\n",
       "..               ...                                  ...\n",
       "110   batch_tr_5.csv          [alp, doruk, selman, gpt4o]\n",
       "111   batch_tr_6.csv          [alp, doruk, selman, gpt4o]\n",
       "112   batch_tr_7.csv          [alp, doruk, selman, gpt4o]\n",
       "113   batch_tr_8.csv          [alp, doruk, selman, gpt4o]\n",
       "114   batch_tr_9.csv          [alp, doruk, selman, gpt4o]\n",
       "\n",
       "[115 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>workerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>batch_ar_1.csv</td>\n",
       "      <td>[husseinsarrar, ilham, nada, gpt4o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>batch_ar_10.csv</td>\n",
       "      <td>[husseinsarrar, ilham, nada, gpt4o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>batch_ar_11.csv</td>\n",
       "      <td>[husseinsarrar, ilham, nada, gpt4o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batch_ar_12.csv</td>\n",
       "      <td>[husseinsarrar, ilham, nada, gpt4o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>batch_ar_13.csv</td>\n",
       "      <td>[husseinsarrar, ilham, nada, gpt4o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>batch_tr_5.csv</td>\n",
       "      <td>[alp, doruk, selman, gpt4o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>batch_tr_6.csv</td>\n",
       "      <td>[alp, doruk, selman, gpt4o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>batch_tr_7.csv</td>\n",
       "      <td>[alp, doruk, selman, gpt4o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>batch_tr_8.csv</td>\n",
       "      <td>[alp, doruk, selman, gpt4o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>batch_tr_9.csv</td>\n",
       "      <td>[alp, doruk, selman, gpt4o]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "LANG_BATCHES = {\n",
    "    'en': [f'batch_en_{i}.csv' for i in range(4,22)],\n",
    "    'nl': [f'batch_nl_{i}.csv' for i in set(range(1,18)) - {3, 16}],\n",
    "    'de': [f'batch_de_{i}.csv' for i in range(1,21)],\n",
    "    'es': [f'batch_es_{i}.csv' for i in range(1,20)],\n",
    "    'tr': [f'batch_tr_{i}.csv' for i in range(1,20)],\n",
    "    'ar': [f'batch_ar_{i}.csv' for i in range(1,20)],\n",
    "}"
   ],
   "metadata": {
    "id": "iJi7xo9Zl4Ji",
    "ExecuteTime": {
     "end_time": "2025-03-07T14:40:54.974416Z",
     "start_time": "2025-03-07T14:40:54.949729Z"
    }
   },
   "outputs": [],
   "execution_count": 194
  },
  {
   "cell_type": "code",
   "source": [
    "# define all the inter-annotator agreements we want to calculate\n",
    "IAA = [\n",
    "    dict(\n",
    "        prefix=f'IAA_toxicity_{key}_',\n",
    "        conditions=[('column_in_list', 'filename', batches)],\n",
    "        to_calculate=['toxicity'],\n",
    "        labels=['Yes/Maybe', 'No']\n",
    "    )\n",
    "    for key, batches in LANG_BATCHES.items()\n",
    "] + [\n",
    "    dict(\n",
    "        prefix=f'IAA_counternarrative_{lang}_',\n",
    "        conditions=[('column_in_list', 'filename', batches)],\n",
    "        to_calculate=['counternarrative'],\n",
    "        labels=COLUMNS['counternarrative'].values,\n",
    "    ) for lang, batches in LANG_BATCHES.items()\n",
    "] + [\n",
    "    dict(\n",
    "        prefix=f'IAA_justInappropriate_{lang}_',\n",
    "        conditions=[\n",
    "            ('column_in_list', 'filename', batches),\n",
    "            ('full_agreement_on_value', 'toxicity', 'Yes/Maybe'),\n",
    "            ('full_agreement_on_value', 'counternarrative', 'No'),\n",
    "        ],\n",
    "        to_calculate=['justInappropriate'],\n",
    "        labels=COLUMNS['justInappropriate'].values,\n",
    "    ) for lang, batches in LANG_BATCHES.items()\n",
    "# ] + [\n",
    "#     dict(\n",
    "#         prefix=f'IAA_other_{lang}_',\n",
    "#         conditions=[\n",
    "#             ('column_in_list', 'filename', batches),\n",
    "#             ('full_agreement_on_value', 'implDetected', True),\n",
    "#             ('full_agreement_on_value', 'hasOther', '[]'),\n",
    "#         ],\n",
    "#         to_calculate=['other'],\n",
    "#         labels=COLUMNS['other'].values,\n",
    "#     ) for lang, batches in LANG_BATCHES.items()\n",
    "] + [\n",
    "    dict(\n",
    "        prefix=f'IAA_subject_{lang}_',\n",
    "        conditions=[\n",
    "            ('column_in_list', 'filename', batches),\n",
    "            ('full_agreement_on_value', 'implDetected', True),\n",
    "        ],\n",
    "        to_calculate=['subject'],\n",
    "        labels=COLUMNS['subject'].values,\n",
    "    ) for lang, batches in LANG_BATCHES.items()\n",
    "] + [\n",
    "    dict(\n",
    "        prefix=f'IAA_implPolarity_{lang}_',\n",
    "        conditions=[\n",
    "            ('column_in_list', 'filename', batches),\n",
    "            ('full_agreement_on_value', 'implDetected', True),\n",
    "        ],\n",
    "        to_calculate=['implPolarity'],\n",
    "        labels=COLUMNS['implPolarity'].values,\n",
    "    ) for lang, batches in LANG_BATCHES.items()\n",
    "] + [\n",
    "    dict(\n",
    "        prefix=f'IAA_implTopic_{lang}_',\n",
    "        conditions=[\n",
    "            ('column_in_list', 'filename', batches),\n",
    "            ('full_agreement_on_value', 'implDetected', True),\n",
    "        ],\n",
    "        to_calculate=['implTopic'],\n",
    "        labels=COLUMNS['implTopic'].values,\n",
    "    ) for lang, batches in LANG_BATCHES.items()\n",
    "] + [\n",
    "    dict(\n",
    "        prefix=f'IAA_implStereotype_{lang}_',\n",
    "        conditions=[\n",
    "            ('column_in_list', 'filename', batches),\n",
    "            ('full_agreement_on_value', 'implDetected', True),\n",
    "        ],\n",
    "        to_calculate=['implStereotype'],\n",
    "        labels=COLUMNS['implStereotype'].values,\n",
    "    ) for lang, batches in LANG_BATCHES.items()\n",
    "] + [\n",
    "    dict(\n",
    "        prefix=f'IAA_implSarcasm_{lang}_',\n",
    "        conditions=[\n",
    "            ('column_in_list', 'filename', batches),\n",
    "            ('full_agreement_on_value', 'implDetected', True),\n",
    "        ],\n",
    "        to_calculate=['implSarcasm'],\n",
    "        labels=COLUMNS['implSarcasm'].values,\n",
    "    ) for lang, batches in LANG_BATCHES.items()\n",
    "# ] + [\n",
    "#     dict(\n",
    "#         prefix=f'IAA_{col}_{lang}_',\n",
    "#         conditions=[\n",
    "#             ('column_in_list', 'filename', batches),\n",
    "#             ('full_agreement_on_value', 'implDetected', True),\n",
    "#         ],\n",
    "#         to_calculate=[col],\n",
    "#         labels=COLUMNS[col].values,\n",
    "#     )\n",
    "#     for lang, batches in LANG_BATCHES.items()\n",
    "#     for col in ['authorBelief', 'authorPrefer', 'authorAccount',\n",
    "#                 'typicalBelief', 'typicalPrefer', 'expertBelief']\n",
    "]\n"
   ],
   "metadata": {
    "id": "Z1m2-dvLISZL",
    "ExecuteTime": {
     "end_time": "2025-03-07T16:17:29.499542Z",
     "start_time": "2025-03-07T16:17:29.471604Z"
    }
   },
   "outputs": [],
   "execution_count": 227
  },
  {
   "cell_type": "code",
   "source": [
    "all_results = []\n",
    "\n",
    "def iaa_calculation(suitable_df, col, key, labels):\n",
    "    col = 'answer_pp_' + col\n",
    "    name = key + col\n",
    "    results = []\n",
    "\n",
    "    print(name)\n",
    "\n",
    "    # calculate cohen's kappa between pairs of annotators\n",
    "    workers = suitable_df['workerid'].unique().tolist()\n",
    "    for w1, w2 in itertools.combinations(workers, r=2):\n",
    "        w1_df = suitable_df.loc[suitable_df['workerid']==w1]\n",
    "        w2_df = suitable_df.loc[suitable_df['workerid']==w2]\n",
    "        valid_comment_ids = (set(w1_df.loc[w1_df[col].notna(), 'comment_id'].unique())\n",
    "                           & set(w2_df.loc[w2_df[col].notna(), 'comment_id'].unique()))\n",
    "        w1_df = w1_df.loc[w1_df['comment_id'].isin(valid_comment_ids)]\n",
    "        w2_df = w2_df.loc[w2_df['comment_id'].isin(valid_comment_ids)]\n",
    "\n",
    "        y1 = w1_df[col]\n",
    "        y2 = w2_df[col]\n",
    "        if len(y1) < len(suitable_df['comment_id'].unique()):\n",
    "            print(f'WARNING reduced support cohen {w1} - {w2}:', len(y1), len(suitable_df['comment_id'].unique()))\n",
    "\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                value = cohen_kappa_score(y1.tolist(), y2.tolist(), labels=labels)\n",
    "\n",
    "            if pd.isna(value):\n",
    "                print(f'WARNING NaN for cohen {w1} - {w2}:', y1.unique(), y2.unique())\n",
    "\n",
    "            results.append({'key': key, 'col': col, 'score': 'cohen', 'w1': w1, 'w2': w2, 'support': len(y1), 'value': value})\n",
    "        except ValueError as e:\n",
    "            print(f'ERROR   {w1} ({len(y1)}), {w2} ({len(y2)})')\n",
    "            ids1 = set(suitable_df.loc[y1.index]['comment_id'].unique())\n",
    "            ids2 = set(suitable_df.loc[y2.index]['comment_id'].unique())\n",
    "            diff1 = ids1 - ids2\n",
    "            print(f'{w1} - {w2}:', suitable_df.loc[suitable_df['comment_id'].isin(diff1), ['filename', 'st_id']])\n",
    "            diff2 = ids2 - ids1\n",
    "            print(f'{w2} - {w1}:',suitable_df.loc[suitable_df['comment_id'].isin(diff2), ['filename', 'st_id']])\n",
    "            raise e\n",
    "\n",
    "    # calculate fleiss' kappa\n",
    "    answers_by_comment = suitable_df.pivot(columns=['workerid'], index=['comment_id'], values=[col])\n",
    "    def fleiss(answers_by_comment, name):\n",
    "        valid = answers_by_comment.notna().all(axis=1)\n",
    "        if valid.sum() < len(valid):\n",
    "            print('WARNING reduced support fleiss:', valid.sum(), len(valid))\n",
    "\n",
    "        table = answers_by_comment.loc[valid].to_numpy()\n",
    "        table, labels = aggregate_raters(table)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            value = fleiss_kappa(table)\n",
    "        results.append({'key': key, 'col': col, 'score': f'fleiss-{name}', 'value': value, 'support': valid.sum()})\n",
    "    fleiss(answers_by_comment, 'all')\n",
    "    fleiss(answers_by_comment.drop(columns=[(col, 'gpt4o')]), 'w/o gpt')\n",
    "\n",
    "    # write answers to csv for manual checking\n",
    "    # answers_by_comment.to_csv('answers_by_comment_' + name + '.csv')\n",
    "\n",
    "    # pprint.pprint(results)\n",
    "    # print('----')\n",
    "\n",
    "    all_results.extend(results)\n",
    "\n",
    "\n",
    "conditional_calculation(IAA, iaa_calculation)\n",
    "results_df = pd.DataFrame.from_dict(all_results)\n",
    "results_df.to_csv('agreement_scores.csv')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpEHU04qM-Og",
    "outputId": "d1137f21-5516-4c24-a1d9-13b2caaa32fb",
    "ExecuteTime": {
     "end_time": "2025-03-07T16:17:43.734767Z",
     "start_time": "2025-03-07T16:17:31.335870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAA_toxicity_en_answer_pp_toxicity\n",
      "IAA_toxicity_nl_answer_pp_toxicity\n",
      "IAA_toxicity_de_answer_pp_toxicity\n",
      "IAA_toxicity_es_answer_pp_toxicity\n",
      "IAA_toxicity_tr_answer_pp_toxicity\n",
      "IAA_toxicity_ar_answer_pp_toxicity\n",
      "IAA_counternarrative_en_answer_pp_counternarrative\n",
      "IAA_counternarrative_nl_answer_pp_counternarrative\n",
      "IAA_counternarrative_de_answer_pp_counternarrative\n",
      "IAA_counternarrative_es_answer_pp_counternarrative\n",
      "IAA_counternarrative_tr_answer_pp_counternarrative\n",
      "IAA_counternarrative_ar_answer_pp_counternarrative\n",
      "IAA_justInappropriate_en_answer_pp_justInappropriate\n",
      "IAA_justInappropriate_nl_answer_pp_justInappropriate\n",
      "WARNING reduced support cohen isa - piek: 29 38\n",
      "WARNING reduced support cohen isa - stefan: 29 38\n",
      "WARNING reduced support cohen isa - gpt4o: 29 38\n",
      "WARNING reduced support fleiss: 29 38\n",
      "WARNING reduced support fleiss: 29 38\n",
      "IAA_justInappropriate_de_answer_pp_justInappropriate\n",
      "WARNING reduced support cohen alexandra_genis - jennystock: 58 59\n",
      "WARNING reduced support cohen alexandra_genis - mfrosin: 57 59\n",
      "WARNING reduced support cohen alexandra_genis - gpt4o: 58 59\n",
      "WARNING reduced support cohen jennystock - mfrosin: 58 59\n",
      "WARNING reduced support cohen mfrosin - gpt4o: 58 59\n",
      "WARNING reduced support fleiss: 57 59\n",
      "WARNING reduced support fleiss: 57 59\n",
      "IAA_justInappropriate_es_answer_pp_justInappropriate\n",
      "WARNING reduced support cohen gpt4o - felipe: 114 121\n",
      "WARNING reduced support cohen angela - felipe: 114 121\n",
      "WARNING reduced support cohen emmako - felipe: 114 121\n",
      "WARNING reduced support fleiss: 114 121\n",
      "WARNING reduced support fleiss: 114 121\n",
      "IAA_justInappropriate_tr_answer_pp_justInappropriate\n",
      "WARNING reduced support cohen alp - doruk: 91 103\n",
      "WARNING reduced support cohen alp - selman: 99 103\n",
      "WARNING reduced support cohen alp - gpt4o: 99 103\n",
      "WARNING reduced support cohen doruk - selman: 94 103\n",
      "WARNING reduced support cohen doruk - gpt4o: 94 103\n",
      "WARNING reduced support fleiss: 91 103\n",
      "WARNING reduced support fleiss: 91 103\n",
      "IAA_justInappropriate_ar_answer_pp_justInappropriate\n",
      "WARNING reduced support cohen husseinsarrar - ilham: 86 87\n",
      "WARNING reduced support cohen husseinsarrar - nada: 84 87\n",
      "WARNING reduced support cohen husseinsarrar - gpt4o: 86 87\n",
      "WARNING reduced support cohen ilham - nada: 85 87\n",
      "WARNING reduced support cohen nada - gpt4o: 85 87\n",
      "WARNING reduced support fleiss: 84 87\n",
      "WARNING reduced support fleiss: 84 87\n",
      "IAA_subject_en_answer_pp_subject\n",
      "WARNING reduced support cohen baran - stefan: 23 24\n",
      "WARNING reduced support cohen ilia - stefan: 23 24\n",
      "WARNING reduced support cohen stefan - gpt4o: 23 24\n",
      "WARNING reduced support fleiss: 23 24\n",
      "WARNING reduced support fleiss: 23 24\n",
      "IAA_subject_nl_answer_pp_subject\n",
      "IAA_subject_de_answer_pp_subject\n",
      "IAA_subject_es_answer_pp_subject\n",
      "IAA_subject_tr_answer_pp_subject\n",
      "WARNING NaN for cohen alp - gpt4o: ['another group'] ['another group']\n",
      "IAA_subject_ar_answer_pp_subject\n",
      "IAA_implPolarity_en_answer_pp_implPolarity\n",
      "WARNING reduced support cohen baran - ilia: 23 24\n",
      "WARNING reduced support cohen baran - stefan: 23 24\n",
      "WARNING reduced support cohen baran - gpt4o: 23 24\n",
      "WARNING reduced support fleiss: 23 24\n",
      "WARNING reduced support fleiss: 23 24\n",
      "IAA_implPolarity_nl_answer_pp_implPolarity\n",
      "WARNING NaN for cohen piek - gpt4o: ['Negative'] ['Negative']\n",
      "IAA_implPolarity_de_answer_pp_implPolarity\n",
      "WARNING NaN for cohen alexandra_genis - jennystock: ['Negative'] ['Negative']\n",
      "WARNING NaN for cohen alexandra_genis - mfrosin: ['Negative'] ['Negative']\n",
      "WARNING NaN for cohen alexandra_genis - gpt4o: ['Negative'] ['Negative']\n",
      "WARNING NaN for cohen jennystock - mfrosin: ['Negative'] ['Negative']\n",
      "WARNING NaN for cohen jennystock - gpt4o: ['Negative'] ['Negative']\n",
      "WARNING NaN for cohen mfrosin - gpt4o: ['Negative'] ['Negative']\n",
      "IAA_implPolarity_es_answer_pp_implPolarity\n",
      "IAA_implPolarity_tr_answer_pp_implPolarity\n",
      "WARNING NaN for cohen doruk - selman: ['Negative'] ['Negative']\n",
      "IAA_implPolarity_ar_answer_pp_implPolarity\n",
      "WARNING NaN for cohen nada - gpt4o: ['Negative'] ['Negative']\n",
      "IAA_implTopic_en_answer_pp_implTopic\n",
      "IAA_implTopic_nl_answer_pp_implTopic\n",
      "IAA_implTopic_de_answer_pp_implTopic\n",
      "IAA_implTopic_es_answer_pp_implTopic\n",
      "WARNING reduced support cohen angela - felipe: 44 45\n",
      "WARNING reduced support cohen emmako - felipe: 44 45\n",
      "WARNING reduced support cohen felipe - gpt4o: 44 45\n",
      "WARNING reduced support fleiss: 44 45\n",
      "WARNING reduced support fleiss: 44 45\n",
      "IAA_implTopic_tr_answer_pp_implTopic\n",
      "IAA_implTopic_ar_answer_pp_implTopic\n",
      "IAA_implStereotype_en_answer_pp_implStereotype\n",
      "IAA_implStereotype_nl_answer_pp_implStereotype\n",
      "IAA_implStereotype_de_answer_pp_implStereotype\n",
      "WARNING reduced support cohen alexandra_genis - mfrosin: 17 18\n",
      "WARNING reduced support cohen jennystock - mfrosin: 17 18\n",
      "WARNING reduced support cohen mfrosin - gpt4o: 17 18\n",
      "WARNING reduced support fleiss: 17 18\n",
      "WARNING reduced support fleiss: 17 18\n",
      "IAA_implStereotype_es_answer_pp_implStereotype\n",
      "IAA_implStereotype_tr_answer_pp_implStereotype\n",
      "IAA_implStereotype_ar_answer_pp_implStereotype\n",
      "IAA_implSarcasm_en_answer_pp_implSarcasm\n",
      "IAA_implSarcasm_nl_answer_pp_implSarcasm\n",
      "IAA_implSarcasm_de_answer_pp_implSarcasm\n",
      "IAA_implSarcasm_es_answer_pp_implSarcasm\n",
      "WARNING reduced support cohen angela - felipe: 44 45\n",
      "WARNING reduced support cohen emmako - felipe: 44 45\n",
      "WARNING reduced support cohen felipe - gpt4o: 44 45\n",
      "WARNING reduced support fleiss: 44 45\n",
      "WARNING reduced support fleiss: 44 45\n",
      "IAA_implSarcasm_tr_answer_pp_implSarcasm\n",
      "WARNING reduced support cohen alp - doruk: 15 16\n",
      "WARNING reduced support cohen doruk - selman: 15 16\n",
      "WARNING reduced support cohen doruk - gpt4o: 15 16\n",
      "WARNING reduced support fleiss: 15 16\n",
      "WARNING reduced support fleiss: 15 16\n",
      "IAA_implSarcasm_ar_answer_pp_implSarcasm\n"
     ]
    }
   ],
   "execution_count": 228
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ..."
   ],
   "metadata": {
    "id": "TmSY5iJzwxvu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ],
   "metadata": {
    "id": "NVeLioFa6VsU",
    "ExecuteTime": {
     "end_time": "2025-03-07T14:41:00.087605Z",
     "start_time": "2025-03-07T14:41:00.062581Z"
    }
   },
   "outputs": [],
   "execution_count": 197
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add majority vote"
   ],
   "metadata": {
    "id": "i4wP9yhiwzl7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def scores_calculation(suitable_df, col, key, labels):\n",
    "    majority = suitable_df[suitable_df['workerid']!='gpt4o'].groupby(by=['comment_id'])[f'answer_{col}'].apply(lambda x: x.mode().iloc[0]).rename('majority')\n",
    "    gpt4o = suitable_df[suitable_df['workerid']=='gpt4o'].set_index('comment_id')[f'answer_{col}'].rename('gpt4o')\n",
    "    both = pd.merge(majority, gpt4o, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    both.to_csv('majority_vs_gpt4o.csv')\n",
    "\n",
    "    y_true = both['majority'].tolist()\n",
    "    y_pred = both['gpt4o'].tolist()\n",
    "\n",
    "    result = precision_recall_fscore_support(y_true, y_pred, labels=labels)\n",
    "    # result = precision_recall_fscore_support(y_true, y_pred)\n",
    "    for key, val in zip(['precis', 'recall', 'fscore', 'suppor'], result):\n",
    "        print(f'{key}: {val}')\n",
    "\n",
    "    print('---------')\n",
    "\n",
    "conditional_calculation(IAA, scores_calculation)"
   ],
   "metadata": {
    "id": "MgnPSfILbXVX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f13c2fd6-0aa9-4b21-97ed-afa769d84b2d",
    "ExecuteTime": {
     "end_time": "2025-03-07T15:39:45.658722Z",
     "start_time": "2025-03-07T15:39:40.623562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precis: [0.81884058 0.71612903]\n",
      "recall: [0.56218905 0.89878543]\n",
      "fscore: [0.66666667 0.79712747]\n",
      "suppor: [201 247]\n",
      "---------\n",
      "precis: [0.68269231 0.80988593]\n",
      "recall: [0.58677686 0.86585366]\n",
      "fscore: [0.63111111 0.83693517]\n",
      "suppor: [121 246]\n",
      "---------\n",
      "precis: [0.95419847 0.64      ]\n",
      "recall: [0.48076923 0.97560976]\n",
      "fscore: [0.63938619 0.77294686]\n",
      "suppor: [260 246]\n",
      "---------\n",
      "precis: [0.94219653 0.68944099]\n",
      "recall: [0.61977186 0.95689655]\n",
      "fscore: [0.74770642 0.80144404]\n",
      "suppor: [263 232]\n",
      "---------\n",
      "precis: [0.89361702 0.8129771 ]\n",
      "recall: [0.77419355 0.91416309]\n",
      "fscore: [0.82962963 0.86060606]\n",
      "suppor: [217 233]\n",
      "---------\n",
      "precis: [0.76303318 0.82372881]\n",
      "recall: [0.75586854 0.82935154]\n",
      "fscore: [0.75943396 0.82653061]\n",
      "suppor: [213 293]\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precis: [0.         0.         1.         0.95652174 0.        ]\n",
      "recall: [0. 0. 1. 1. 0.]\n",
      "fscore: [0.         0.         1.         0.97777778 0.        ]\n",
      "suppor: [ 0  1  1 22  0]\n",
      "---------\n",
      "precis: [0.         0.71428571 1.         0.84615385 0.        ]\n",
      "recall: [0.         0.71428571 1.         0.84615385 0.        ]\n",
      "fscore: [0.         0.71428571 1.         0.84615385 0.        ]\n",
      "suppor: [ 0  7  3 13  0]\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precis: [0.     0.     0.     0.9375 0.    ]\n",
      "recall: [0.     0.     0.     0.9375 0.    ]\n",
      "fscore: [0.     0.     0.     0.9375 0.    ]\n",
      "suppor: [ 0  1  1 16  0]\n",
      "---------\n",
      "precis: [1.         0.5        0.66666667 0.96969697 0.        ]\n",
      "recall: [1.         1.         0.57142857 0.91428571 0.        ]\n",
      "fscore: [1.         0.66666667 0.61538462 0.94117647 0.        ]\n",
      "suppor: [ 1  2  7 35  0]\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precis: [0. 0. 0. 1. 0.]\n",
      "recall: [0. 0. 0. 1. 0.]\n",
      "fscore: [0. 0. 0. 1. 0.]\n",
      "suppor: [ 0  0  0 16  0]\n",
      "---------\n",
      "precis: [0.         0.         0.5        0.76190476 0.        ]\n",
      "recall: [0.         0.         0.5        0.94117647 0.        ]\n",
      "fscore: [0.         0.         0.5        0.84210526 0.        ]\n",
      "suppor: [ 1  4  2 17  0]\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precis: [0.875 0.    0.   ]\n",
      "recall: [1. 0. 0.]\n",
      "fscore: [0.93333333 0.         0.        ]\n",
      "suppor: [21  2  1]\n",
      "---------\n",
      "precis: [1. 0. 0.]\n",
      "recall: [1. 0. 0.]\n",
      "fscore: [1. 0. 0.]\n",
      "suppor: [23  0  0]\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precis: [1. 0. 0.]\n",
      "recall: [1. 0. 0.]\n",
      "fscore: [1. 0. 0.]\n",
      "suppor: [18  0  0]\n",
      "---------\n",
      "precis: [1.         0.         0.33333333]\n",
      "recall: [0.93181818 0.         1.        ]\n",
      "fscore: [0.96470588 0.         0.5       ]\n",
      "suppor: [44  0  1]\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precis: [1. 0. 0.]\n",
      "recall: [0.9375 0.     0.    ]\n",
      "fscore: [0.96774194 0.         0.        ]\n",
      "suppor: [16  0  0]\n",
      "---------\n",
      "precis: [0.95833333 0.         0.        ]\n",
      "recall: [1. 0. 0.]\n",
      "fscore: [0.9787234 0.        0.       ]\n",
      "suppor: [23  0  1]\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precis: [0. 0. 0. 0. 0. 0. 0.]\n",
      "recall: [0. 0. 0. 0. 0. 0. 0.]\n",
      "fscore: [0. 0. 0. 0. 0. 0. 0.]\n",
      "suppor: [0 0 0 0 0 0 0]\n",
      "---------\n",
      "precis: [0. 0. 0. 0. 0. 0. 0.]\n",
      "recall: [0. 0. 0. 0. 0. 0. 0.]\n",
      "fscore: [0. 0. 0. 0. 0. 0. 0.]\n",
      "suppor: [0 0 0 0 0 0 0]\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precis: [0. 0. 0. 0. 0. 0. 0.]\n",
      "recall: [0. 0. 0. 0. 0. 0. 0.]\n",
      "fscore: [0. 0. 0. 0. 0. 0. 0.]\n",
      "suppor: [0 0 0 0 0 0 0]\n",
      "---------\n",
      "precis: [0. 0. 0. 0. 0. 0. 0.]\n",
      "recall: [0. 0. 0. 0. 0. 0. 0.]\n",
      "fscore: [0. 0. 0. 0. 0. 0. 0.]\n",
      "suppor: [0 0 0 0 0 0 0]\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precis: [0. 0. 0. 0. 0. 0. 0.]\n",
      "recall: [0. 0. 0. 0. 0. 0. 0.]\n",
      "fscore: [0. 0. 0. 0. 0. 0. 0.]\n",
      "suppor: [0 0 0 0 0 0 0]\n",
      "---------\n",
      "precis: [0. 0. 0. 0. 0. 0. 0.]\n",
      "recall: [0. 0. 0. 0. 0. 0. 0.]\n",
      "fscore: [0. 0. 0. 0. 0. 0. 0.]\n",
      "suppor: [0 0 0 0 0 0 0]\n",
      "---------\n",
      "precis: [0.11111111 1.        ]\n",
      "recall: [1.         0.27272727]\n",
      "fscore: [0.2        0.42857143]\n",
      "suppor: [ 2 22]\n",
      "---------\n",
      "precis: [0.3 1. ]\n",
      "recall: [1.   0.65]\n",
      "fscore: [0.46153846 0.78787879]\n",
      "suppor: [ 3 20]\n",
      "---------\n",
      "precis: [0.85714286 0.75      ]\n",
      "recall: [0.92307692 0.6       ]\n",
      "fscore: [0.88888889 0.66666667]\n",
      "suppor: [13  5]\n",
      "---------\n",
      "precis: [0.8 0.8]\n",
      "recall: [0.76190476 0.83333333]\n",
      "fscore: [0.7804878  0.81632653]\n",
      "suppor: [21 24]\n",
      "---------\n",
      "precis: [0.6        0.83333333]\n",
      "recall: [0.85714286 0.55555556]\n",
      "fscore: [0.70588235 0.66666667]\n",
      "suppor: [7 9]\n",
      "---------\n",
      "precis: [0.14285714 0.9       ]\n",
      "recall: [0.66666667 0.42857143]\n",
      "fscore: [0.23529412 0.58064516]\n",
      "suppor: [ 3 21]\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Projects/toxic-reasoning/env-models/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precis: [0. 1.]\n",
      "recall: [0.         0.95833333]\n",
      "fscore: [0.        0.9787234]\n",
      "suppor: [ 0 24]\n",
      "---------\n",
      "precis: [0.33333333 1.        ]\n",
      "recall: [1.         0.90909091]\n",
      "fscore: [0.5        0.95238095]\n",
      "suppor: [ 1 22]\n",
      "---------\n",
      "precis: [0.5    0.9375]\n",
      "recall: [0.5    0.9375]\n",
      "fscore: [0.5    0.9375]\n",
      "suppor: [ 2 16]\n",
      "---------\n",
      "precis: [1.         0.85714286]\n",
      "recall: [0.33333333 1.        ]\n",
      "fscore: [0.5        0.92307692]\n",
      "suppor: [ 9 36]\n",
      "---------\n",
      "precis: [1. 1.]\n",
      "recall: [1. 1.]\n",
      "fscore: [1. 1.]\n",
      "suppor: [ 1 15]\n",
      "---------\n",
      "precis: [0.         0.95454545]\n",
      "recall: [0.         0.91304348]\n",
      "fscore: [0.         0.93333333]\n",
      "suppor: [ 1 23]\n",
      "---------\n"
     ]
    }
   ],
   "execution_count": 208
  }
 ]
}
